.. role:: lamdablue
    :class: lamdablue

.. role:: lamdaorange
    :class: lamdaorange

.. raw:: html

    <style>

    .lamdablue {
        color: #47479e;
        font-weight: bold;
    }
    .lamdaorange {
        color: #fd4d01;
        font-weight: bold;
    }
    .customcolor1 {
        color: #f48702;
        font-weight: bold;
    }
    .customcolor2 {
        color: #f64600;
        font-weight: bold;
    }
    .customcolor3 {
        color: #de1500;
        font-weight: bold;
    }
    .customcolor4 {
        color: #b70501;
        font-weight: bold;
    }
    .customcolor5 {
        color: #d6005c;
        font-weight: bold;
    }
    .lamdablue {
        color: #47479e;
        font-weight: bold;
    }
    .lamdaorange {
        color: #fd4d01;
        font-weight: bold;
    }
    .customlyellow {
        color: #00ff73;
    }

    </style>




Config with ~1 Line Blitz
=================================

.. raw:: html

   <span style="font-size: 25px;">ğŸŒ±</span>
   <p></p>


:lamdaorange:`Z`:lamdablue:`h`:lamdablue:`i`:lamdaorange:`J`:lamdablue:`i`:lamdablue:`a`:lamdablue:`n` is an **unifying** and **rapidly deployable** toolbox for **pre-trained model reuse**.

- **What** \& **Why** Reuse?

  - Performing downstream tasks **with the help of pre-trained model**, including model structures, weights, or other derived rules.
  - Significantly **accelerating convergence** and **improving downstream performance**.

In :lamdaorange:`Z`:lamdablue:`h`:lamdablue:`i`:lamdaorange:`J`:lamdablue:`i`:lamdablue:`a`:lamdablue:`n`, adding the **LoRA** module to the pre-trained model and adjusting which part of the parameters to fine-tune just require **about** :customlyellow:`one` **line of code.**


Overview
-------------------------

:customcolor5:`In` :customcolor4:`the` :customcolor2:`following` :customcolor1:`example`, we show how :lamdaorange:`Z`:lamdablue:`h`:lamdablue:`i`:lamdaorange:`J`:lamdablue:`i`:lamdablue:`a`:lamdablue:`n`:

  + Represent the modules of the pre-trained model
  + Config the extended add-in module with entry points


Modules of Pre-trained Model in One Line description
-------------------------

'(LoRA.adapt): ...->(blocks[0:12].attn.qkv){inout1}->...',

åœ¨Architectæ¨¡å—ä¸­ï¼Œä¸ºäº†æ›´æ–¹ä¾¿åœ°ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œä¸ºé¢„è®­ç»ƒæ¨¡å‹æ·»åŠ é¢å¤–çš„é€‚é…ç»“æ„ï¼ŒZhiJianæ¥å—å•è¡Œåºåˆ—åŒ–åœ°è¡¨ç¤ºåŸºç¡€çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾‹å¦‚ä¸‹å›¾çš„timmåº“ä¸­çš„Vision Transformeræ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å¦‚ä¸‹çš„å•è¡Œé…ç½®æ¥è¡¨ç¤ºå®ƒï¼š

:lamdaorange:`Z`:lamdablue:`h`:lamdablue:`i`:lamdaorange:`J`:lamdablue:`i`:lamdablue:`a`:lamdablue:`n`

å…¶ä¸­()æ‹¬å·å†…è¡¨ç¤ºåŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„æ¨¡å—ï¼Œåœ¨æ‹¬å·å†…æ”¯æŒç”¨'.'è®¿é—®ç¬¦æ¥TODO

ç®­å¤´è¡¨ç¤ºæ¨¡å—é—´çš„è¿æ¥ï¼Œ...è¡¨ç¤ºç¼ºçœçš„æ¨¡å—ï¼Œå…è®¸ç”¨ç®­å¤´è¿æ¥éƒ¨åˆ†ç»“æ„ï¼Œå¦‚ä¸Šè¿°ä»£ç æ‰€ç¤ºï¼Œæˆ‘ä»¬åªè¡¨ç¤ºäº†blocks TODOæ¨¡å—


Extended Add-in Module with Entry Points
-------------------------

æˆ‘ä»¬ç”¨(): æ¥è¡¨ç¤ºä¸€ä¸ªé¢å¤–çš„é€‚é…ç»“æ„ï¼Œå…¶ä¸­'.'ä¹‹åæ˜¯é¢å¤–ç»“æ„çš„ä¸»è¦forwardå‡½æ•°ï¼Œæ•°æ®æµå…¥æ¨¡å—åå°†ä¸»è¦ç»è¿‡è¯¥æ–¹æ³•ã€‚

æˆ‘ä»¬ç”¨{}æ¥è¡¨ç¤ºé¢å¤–ç»“æ„å¯¹äºé¢„è®­ç»ƒæ¨¡å‹çš„æ¥å…¥ç‚¹ï¼ŒåŒ…æ‹¬æºæ¨¡å‹ç‰¹å¾çš„å…¥å£å’Œæ·»åŠ ç»“æ„å¤„ç†åç‰¹å¾çš„è¿”å›ç‚¹


æ€»ç»“
-------------------------

å¦‚ä¸Šé…ç½®åï¼ŒZhiJianæ”¯æŒæ— ç¼åœ°ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹çš„ç»“æ„ï¼Œå®ƒå°†è‡ªåŠ¨è¯†åˆ«å®šä¹‰åœ¨:code:`zhijian\models\addin`ä¸­çš„é¢å¤–ç»“æ„ï¼Œå®Œæˆé¢„è®­ç»ƒç½‘ç»œçš„æ­å»º
